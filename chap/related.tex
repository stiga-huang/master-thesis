% Copyright (c) 2014,2016 Casper Ti. Vector
% Public domain.

\chapter{相关工作}  \label{chap:related}
\section{分布式数据库HBase}
HBase是Google BigTable\supercite{bigtable}的开源实现 ，是Hadoop 生态系统中一个列式NoSQL数据库，搭建在HDFS（Hadoop分布式文件系统）之上。HBase由于其优异的可扩展性和稳定性，在产业界得到了广泛应用。HBase也是图数据库Titan最常使用的存储引擎，为了介绍Titan的底层实现，此处先详述HBase的相关实现。
下面分别介绍HBase的数据模型、操作接口、存储实现以及HBase与同类系统的对比。

\subsection{HBase数据模型}
在HBase中，数据首先以表为单位进行组织。数据表以行为单位组织，每行由一个键值唯一标识，称为行键（row key）。一行中可以包含任意列的数据，每列对应的数据可分为多个版本（对应多个时间戳）。每个版本的数据是最小粒度的数据单元，称为一个单元格（cell）。因此一个单元格由行键、列名、版本号（时间戳）和值（value）组成。
在HBase中，各列按列族（column family）进行组织，因此列名实际由列族名和列修饰符（column qualifier）组成。区别于传统关系模型，列族里的列修饰符可以随时添加，不需要事先定义。即在插入数据时，可以指定任意的列修饰符。行键是每个表必须要有的一列，且只能是唯一的一列，不属于任何列族。图\ref{fig:hbase_table}是HBase中一张数据表的示例。

\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{fig/HBase_table_example.pdf}
\caption{HBase数据表示例}
\label{fig:hbase_table}
\end{figure}

HBase也被认为是一个键值存储（Key-Value Store），相当于数据结构Map的存储。这是因为每个单元格实际存储的是Value部分，而Key部分就是由行键、列族名、列修饰符和时间戳合并构成。由于列修饰符是不需要事先定义的，因此整个Key部分是可以随意指定的，从而可以将HBase归类为键值存储。图\ref{fig:hbase_cell}是一个Cell的数据组成以及如何表示为键值对的示例。单元格也确实是HBase的存储实现中的最小单元，因此将HBase归类为键值存储。

\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{fig/HBase_key_value.pdf}
\caption{HBase中Cell单元的Key、Value成分}
\label{fig:hbase_cell}
\end{figure}

\subsection{HBase操作接口}
HBase提供了一个交互式的shell作为操作接口，同时也提供原生的Java接口，另外还提供了thrift API，支持C++、Python、Ruby、Perl、PHP等语言。下面以HBase shell为例叙述HBase的操作接口。

HBase建表时需要指定表中的所有列族，下面的操作创建了名为table1的数据表，它拥有两个列族cf1和cf2：
\begin{verbatim}
HBase Shell > create 'table1', 'cf1', 'cf2'
\end{verbatim}

在建表之后还可以增加或删除列族，也可以修改列族的配置，但需要disable整个表，即中止所有读写请求，操作结束后再enable该表：
\begin{verbatim}
HBase Shell > disable 'table1'
HBase Shell > alter 'table1', {NAME => 'cf2', VERSIONS => 5}
HBase Shell > alter 'table1', 'delete' => 'cf1'
HBase Shell > alter 'table1', 'cf3'
HBase Shell > enable 'table1'
\end{verbatim}
上面的操作设置列族cf2的最大版本数为5，即每列最多存储5个版本的数据。下一行删除列族cf1及其下的所有数据。之后添加一个列族cf3。

HBase提供Put、Get、Scan、Delete接口进行数据的增删查改。Put接口给某一行中插入一个单元格，需要指定行键、列族和列修饰符。时间戳则是可选的，若没有指定，则使用插入时间作为时间戳。
\begin{verbatim}
HBase Shell > create 'table1', 'cf1', 'cf2'
HBase Shell > put 'table1', 'row1', 'cf1:a', 'value1'
HBase Shell > put 'table1', 'row2', 'cf2:b', 'value2'
HBase Shell > put 'table1', 'row1', 'cf1:b', 'value3', timestamp1
\end{verbatim}

Delete接口跟Put接口相反，用来删除一个单元格的数据。由于参数相同，这里不再举例。

Get接口跟Scan接口用来读取数据，其中Get接口用来读取一行内的数据，Scan接口用来读取多行的数据。Get接口只需给定一个行键，参数里的列族、列修饰符和时间戳都是可选的，会把满足条件的所有单元格都返回。Get接口能读取的范围不超过一行，即Get接口可以读取一个单元格，也可以读取一行中所有的单元格，但读取的对象都只能在同一行中。下面是一些读取示例：
\begin{verbatim}
HBase Shell > get 'table1', 'row1'
HBase Shell > get 'table1', 'row1', 'cf2:a'
HBase Shell > get 'table1', 'row1', {COLUMN => ['cf:a', 'cf:b', 'cf:c']}
HBase Shell > get 'table1', 'row1', {COLUMN => 'cf:c', TIMESTAMP => ts1}
\end{verbatim}

Scan接口则用来读取连续的多行数据。在HBase数据表中各行是按行键升序排列的，因此Scan时只需给定一个起始行键和一个终止行键（不包含），就可以返回所有的这些行。如果不给定范围，则Scan接口返回表中的所有数据。Scan接口也可指定特定的列或附加Filter来过滤无关数据。下面是一些操作示例：
\begin{verbatim}
HBase Shell > scan 'table1', {LIMIT => 100}
HBase Shell > scan 'table1', {COLUMNS => ['cf1:a', 'cf1:b'], LIMIT => 10,
                              STARTROW => 'xyz'}
\end{verbatim}
第一行读取表中的100行数据，第二行从行键大于等于'xyz'的行中读取列族为cf1，列修饰符为a或b的10行数据。

之所以要区分Get和Scan接口，是因为HBase只保证单行数据的原子性，即Get接口不会返回部分更新的数据，但Scan接口没有这个保证。
在本节也可以看到，HBase并没有提供类型关系数据库的SQL接口，这是所有NoSQL数据库的一个特点，都是根据自己的数据模型因地制宜地提供对应的接口。

% ACID的支持情况是否要说？

\subsection{HBase存储实现}
HBase集群采用Master-Slave架构，其中Master节点为HMaster，Slave节点为RegionServer，每台工作机器只需部署一个RegionServer。
HMaster和RegionServer之间通过ZooKeeper\footnote{ZooKeeper, http://zookeeper.apache.org/}来协同工作。HMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个HMaster在运行。

\begin{figure}[htbp]
\centering
\includegraphics[width=100mm]{fig/big_table.pdf}
\caption{BigTable模型示例}
\label{fig:big_table}
\end{figure}

RegionServer负责管理数据表的分片（Region）。HBase采用BigTable数据模型（如图\ref{fig:big_table}）。在BigTable数据表中，各行首先按行键升序排列，行内的各单元格再依次按列名、时间戳排序。作为一个列式存储数据库，HBase中不同列族的数据是分开存储的。由于行键有序，因此可以装数据表水平切分为不相交的Region，每个Region由一个左闭右开的行键区间来表示。每个Region交由一个RegionServer来管理，RegionServer进一步将这些数据存储在HDFS中，并响应这个Region上的读写请求。

\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{fig/HBase_region.pdf}
\caption{HBase Region内部结构}
\label{fig:hbase_region}
\end{figure}

每个Region在RegionServer内的组成成分如图\ref{fig:hbase_region}所示。
Region内部先按列族分为不同的HStore，每个HStore采用LSM Tree\supercite{LSM_tree}数据结构进行存储。在LSM Tree中，数据更新先写入内存缓冲区MemStore中，在MemStore中数据是有序排列的。当MemStore写满时，会将数据flush进磁盘成为文件，即一个StoreFile。由于生成StoreFile时是磁盘的顺序写，因此具有很高的性能。HBase利用LSM Tree数据结构实现了高效的随机写入。然而给定一个Key值要进行读取时，需要在MemStore和所有StoreFile中进行查找，如果StoreFile数目过多将会影响读取速度。RegionServer有线程会定期对HStore里的StoreFile进行合并（compaction），以降低StoreFile的数目，减小读请求的开销。

最后需要补充的是，每个RegionServer会维护一个WAL（Write Ahead Log）文件，写请求在写入MemStore前都需要先写入WAL中，以保证宕机时内存中的数据不会丢失。WAL和底层的StoreFile都是存储在HDFS上的，具有多复本的容错机制。当某台 RegionServer 宕机时，HMaster会将这些文件分配给其它RegionServer进行恢复，从而实现容错性（Fault Tolerance）。

\subsection{HBase与同类系统对比}
在分布式键值存储中，另一个著名的系统是Cassandra\supercite{cassandra}。Cassandra是Amazon DynamoDB\supercite{DynamoDB}的开源实现，最初由Facebook开发并开源。

区别于BigTable中按字典序来划分Key值空间，Cassandra中是按Key值的哈希值来划分空间的。另外Cassandra不依赖于HDFS进行存储，其数据直接由各Server存储在本地磁盘中，Server之间对数据保留多个复本。由于Key值的寻址逻辑更为简单，以及更为直接的存储实现，Cassandra相比HBase具有更优的读写性能。

在系统架构方面，Cassandra使用去中心化的系统架构，没有Master节点，因此完全除去了单点故障的可能。所有Server组成一个环状结构，使用一致性哈希算法来分配Key值空间。一致性哈希算法使得集群可以方便地添加或删除节点。区别于HBase依赖ZooKeeper进行协作，Cassandra中的Server之间使用Gossip协议进行协作。

Cassandra与HBase最大的区别在于CAP\supercite{CAP}理论上的取舍。CAP是Consistency、 Availability、 Partition tolerance的简称，表示分布式系统的三个性能。Consistency是指系统的读请求总能返回最新的写结果，即读写请求是一致的。Availability是指系统一直处于可用的状态，不会因为宕机等原因造成一段时间不可服务。Partition tolerance是指分布式系统因网络故障等被切分为不可通信的几部分时仍能正常工作。CAP理论指出，任何系统最多只能满足三条性能中的两条，不可能三者兼备。

HBase和Cassandra都是分布式的系统，在部分机器宕机或失联时系统仍能工作，都是满足Partition tolerance的。它们的区别在于Consistency和Availability的取舍上。
HBase 在有RegionServer宕机时，需要让相应的WAL和StoreFile在其它RegionServer上进行恢复，在恢复期间对应的Region是无法提供服务的，因此不满足Availability。但也正得益于这种恢复机制，HBase中的数据会一直处于一致的状态，即HBase满足Consistency。
Cassandra由于不依赖HDFS之类的可靠存储层，需要自身在各Server间对数据做多复本存储。而读请求是可以由单独一台Server来响应地，不需要等待各Server间的同步，因此也就没有了一致性的保证，按来了读写的高性能。

\section{图数据库Titan}
图数据库是以图的形式来表示和管理数据的数据库\supercite{graph_models_survey}，与传统的关系型数据库相比，图数据库在图结构相关的查询上有更优异的性能，如多跳邻域查询、路径查询、局部聚集系数计算等。
Titan是一个基于Blueprints 接口设计的开源图数据库，其实现了一个可插拔的存储接口，可以部署在BerkerlyDB 、HBase或Cassandra之上。相比于著名的图数据库Neo4j，Titan是完全开源免费的，其受关注度正在与日俱增。而且由于Hadoop生态系统在产业界的广泛应用，在HBase上搭建Titan，即使用Titan on HBase应对图处理需求是较为常见的选择。

\subsection{Titan存储实现}
在图数据库Titan中，基于BigTable模型，数据在HBase中以邻接表的形式存储，如图\ref{fig:adj_list}所示。

\begin{figure}[htbp]
\centering
\includegraphics[width=100mm]{fig/adj_list.pdf}
\caption{Titan内部的BigTable实现}
\label{fig:adj_list}
\end{figure}

Titan为每个点分配了一个全局唯一的id，每个点占BigTable中的一行，行键就是点的id。每行存储了该点相关的属性和边，它们各占一个单元格。Titan在BigTable模型中设置最大版本数为1，从而使每行中的列名与单元格一一对应，根据行键和列名可以快速定位到目标单元格，实现对边和属性内容的快速检索。

在Titan中，每个属性是一个key-value对。点的属性存储在该点所在的行，每个属性占一个单元格，并以属性名key作为列名，这使得每个点的属性查询可以非常高效。

每个点所在的行还存储了邻接的所有边数据，每条边占一个单元格。一条边的信息包含了邻接点、类别（label）、方向、边的唯一id，以及边上的各属性。单元格的值用来存储边上的所有属性，列名则存储边上除属性外的其它信息。在BigTable模型中，同一行的单元格按列名排序。为了方便检索，每条边所在的单元格依次以label、方向、邻接点id以及边id拼接成为列名，即
\begin{center}
  列名 = 边label + 方向 + adjVid + eid
\end{center}
这种列名设计使得一个点的所有边先按label排序，再按方向、邻接点id、边id等排序。其优点是当要检索该点指定label的所有边时，只需要扫描邻接表的一部分。图4是一个更详细的示例。

\subsection{图数据库Titan的局限性}
当图中的重边数量巨大时，Titan的邻接表列数也急剧变大，这会对邻域中点和边的检索带来严重影响。

\begin{figure}[htbp]
\centering
\includegraphics[width=150mm]{fig/original_list.pdf}
\caption{当属性图富含重边时，Titan在HBase中的数据表是一张扁平而宽的表}
\label{fig:orginal_list}
\end{figure}

图\ref{fig:orginal_list}展示了一个富含重边的场景，为方便展示省略了边的方向。v1只有v2和v3两个邻接点，其中与v2有两种label的边，与v3有一种label的边。虽然邻接的点数不多，但v1与邻接点都有数量巨大的重边。由于邻接表存储的是每条边的信息，当要查询v1的所有邻接点集（即v1、v2）时，不得不遍历一次v1的所有边，即遍历整个邻接表。当重边数量巨大时，这是大量的无谓开销。

为了规避上述情形，我们可以换一种列名设计来优化邻域点集查询，比如让邻接表先按邻接点id排序，令
\begin{center}
  列名 = adjVid + 边label + 方向 + eid
\end{center}
这样当发现一个邻接点时，可以跳过相同邻接点的所有边，不用再遍历整个邻接表。然而，面对label相关的查询时又会面临同样的问题，比如查询该点总共有几种label的边，仍需要遍历该点的整个邻接表。因此，改变邻接表的列名设计并不能解决问题，本质原因是邻接表存储了所有的边集。

另一方面，Titan为加快数据访问设计了缓存，存储最近访问的点及其邻接表内容。如果图中各点的重边都很多，则缓存空间被大量的重边占满。由于是重边，这些边中只有为数不多的邻接点。在做邻域点集相关查询时就会极大增加缓存的失效率。
面对富含重边的属性图，把所有边集都存在邻接表中是Titan性能受损的原因所在。

% vim:ts=4:sw=4
